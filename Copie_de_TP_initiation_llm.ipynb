{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardsebastien/TP_initiation_LLM/blob/main/Copie_de_TP_initiation_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_PjGE31q6E"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincentmartin/tp-initiation-llm-student-version/blob/main/TP_initiation_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a72qY8gX1q6G"
      },
      "source": [
        "# TP d'initiation aux LLM\n",
        "\n",
        "Dans ce TP, vous allez apprendre les bases de l'IA générative en manipulant et en contrôlant un LLM installé en local.\n",
        "\n",
        "En sortie de ce module, vous serez capable de :\n",
        "- Installer et importer les dépendances nécessaires\n",
        "- Interroger un LLM pour répondre à tout type de question, comme avec chatGPT\n",
        "- Analyser le fonctionnement d'un LLM\n",
        "- Utiliser un LLM pour résumer une conversation\n",
        "- Explorer les techniques de zero-shot, one-shot et few-shot inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YDdjJHI1q6G"
      },
      "source": [
        "### Instruction à suivre pour exécution sur Google Colab\n",
        "\n",
        "Aller dans `Execution -> Modifier le type d'exécution` puis sélectionner `T4-GPU` pour exploiter les fonctionnalités GPU.\n",
        "\n",
        "![Colab GPU](resources/colab_gpu.png \"T4-GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOTceXaN1q6G"
      },
      "source": [
        "## Installation des dépendances\n",
        "\n",
        "Installons les dépendances nécessaires :\n",
        "- **transformers** : la bibliothèque permettant de mettre en oeuvre les LLM exploitant le modèle transformers\n",
        "- **torch** : célèbre bibliothèque de deep learning, sous jacente à transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjaeX2DJ1q6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6325153e-463c-4301-b20b-997e9320acc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.8.30)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U datasets\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch \\\n",
        "    torchdata\n",
        "\n",
        "%pip install \\\n",
        "    transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPDoQtaW1q6H"
      },
      "source": [
        "Chargeons les dépendances.\n",
        "\n",
        "**Remarque : Si l'exécution ressort en erreur ; tenter de recharger les dépendances.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKtC8KIF1q6H"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNbWrEWc1q6I"
      },
      "source": [
        "## Chargement du LLM\n",
        "\n",
        "Pour interagir avec un LLM, nous allons d'abord devoir le télécharger. Pour cet exemple, nous choissons un modèle simple et \"léger\" : flan-t5.\n",
        "\n",
        "Nous chargeons également le **Tokenizer** afin de convertir le texte en tokens et vice-versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHNBEuVz1q6I"
      },
      "outputs": [],
      "source": [
        "model_name='google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN_ojl011q6I"
      },
      "source": [
        "**Exercice** : en vous aidant de la documentation https://huggingface.co/docs/transformers/llm_tutorial :\n",
        "- Générer et afficher les tokens (ids) de la phrase (encodage)\n",
        "- Décoder la liste de tokens (ids) et afficher la phrase (décodage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukAWnixf1q6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dc4c01-81f8-477c-a20a-e89bf15bdeb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 7227,     3, 16162,    18,    17,    76,   140,  5794,   244,   110,\n",
            "           301, 11160,     7,     3,    58,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "Que peux-tu me dire sur les LLMs ?\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Que peux-tu me dire sur les LLMs ?\"\n",
        "\n",
        "# Encoder la phrase en tokens : suite d'ID ; 1 ID = 1 token\n",
        "# METTRE ICI LE CODE POUR ENCODER UNE PHRASE EN TOKEN ET L'AFFICHER\n",
        "model_inputs = tokenizer(sentence, return_tensors='pt')\n",
        "print(model_inputs)\n",
        "# METTRE ICI LE CODE POUR DECOER UNE SEQUENCE D'ID EN PHRASE ET L'AFFICHER\n",
        "print(tokenizer.decode(model_inputs[\"input_ids\"][0],skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3I4bux11q6I"
      },
      "source": [
        "## Interrogation du LLM\n",
        "\n",
        "A présent, utilisons notre LLM pour générer du texte.\n",
        "\n",
        "Notez la syntaxe `User: question? Assistant: \"`. Nous utilisons cette syntaxe car le LLM est un modèle qui génère la suite de la phrase et cette syntaxe lui permet de comprendre ce qu'on attend de lui. Ceci à la différence des modèles d'instruction qui génèrent une réponse pour une instruction donnée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBBVgK781q6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81fe69db-8c7c-4362-8d07-900f048ee525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris\n"
          ]
        }
      ],
      "source": [
        "sentence = \"User: quelle est la capitale de la france ?Assistant: \"\n",
        "\n",
        "inputs = tokenizer(sentence, return_tensors='pt') # return les tenseurs au format pytorch\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50\n",
        "    )[0],\n",
        "    skip_special_tokens=True # Ne pas retourner les tokens <s>, </s>, ...\n",
        ")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7I_f_ci1q6J"
      },
      "source": [
        "C'est assez basique pour l'instant mais ne vous inquiétez pas, ce n'est que le premier TP ;)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eILj58B1q6J"
      },
      "source": [
        "## Résumé de dialogues\n",
        "\n",
        "Dans cette partie, nous allons utiliser le LLM pour résumer des dialogues.\n",
        "\n",
        "Tout d'abord, téléchargeons le dataset [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) depuis Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkD8HbiF1q6J"
      },
      "outputs": [],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvuwBzts1q6J"
      },
      "source": [
        "Affichons 2 exemples de dialogues, les exemples numéro 40 et 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKjwiIZT1q6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c464fa-2552-4f01-9322-004b2f3c4510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE:\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME HUMAIN:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE:\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME HUMAIN:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_indices = [40, 200]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print('DIALOGUE D ENTREE:')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('RESUME HUMAIN:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YE_-Es_1q6J"
      },
      "source": [
        "Tentons une première approche pour résumer les dialogues 40 et 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1I4kPOk1q6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d66e5d-7ce5-4659-a134-5af934c0dbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE::\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME HUMAIN:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME PAR LLM SANS PROMPT ENGINEERING:\n",
            "Person1: It's ten to nine.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE::\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME HUMAIN:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME PAR LLM SANS PROMPT ENGINEERING:\n",
            "#Person1#: I'm thinking of upgrading my computer.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt') # retourner les tenseurs\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50, # max 50 tokens générés\n",
        "        )[0],\n",
        "        skip_special_tokens=True # on ne génère pas les tokens spéciaux <, >, ...\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'DIALOGUE D ENTREE::\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'RESUME HUMAIN:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'RESUME PAR LLM SANS PROMPT ENGINEERING:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GBpoFfH1q6J"
      },
      "source": [
        "**Exercice** : selon vous est-ce que le résumé est bon ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf-Xcjl41q6J"
      },
      "source": [
        "## Résumé avec un prompt Instruction\n",
        "\n",
        "Dans l'exemple ci-dessous, ajoutons une instruction indiquant au LLM ce qu'il doit faire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mimYW-Fz1q6J"
      },
      "source": [
        "### 1. Zero shot inference\n",
        "\n",
        "Pour amener le modèle à accomplir une tâche, comme résumer un dialogue, vous pouvez transformer ce dialogue en une consigne spécifique. Cela est connu sous le nom d'inférence zéro-shot.\n",
        "\n",
        "En encadrant le dialogue dans une consigne descriptive, vous pourrez observer les modifications apportées au texte généré."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fuPhHE21q6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6f4ab7-9e2b-4d80-8586-4d9272f61e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "Example  1\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE:\n",
            "\n",
            "Summarize the following conversation between two persons to extract the key points of the conversation.\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESME HUMAIN:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "---------------------------------------------------------------------------------------------------\n",
            ">>>RESME AVEC ZERO SHOT INFERENCE:\n",
            "The train is about to leave, but Tom has to catch it.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Example  2\n",
            "---------------------------------------------------------------------------------------------------\n",
            "DIALOGUE D ENTREE:\n",
            "\n",
            "Summarize the following conversation between two persons to extract the key points of the conversation.\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Summary:\n",
            "    \n",
            "---------------------------------------------------------------------------------------------------\n",
            "RESME HUMAIN:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "---------------------------------------------------------------------------------------------------\n",
            ">>>RESME AVEC ZERO SHOT INFERENCE:\n",
            "#Person1: Have you considered upgrading your system? #Person2: Yes, but I'm not sure what exactly I would need. #Person1: You could add a painting program to your software. #Person\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation between two persons to extract the key points of the conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'DIALOGUE D ENTREE:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'RESME HUMAIN:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'>>>RESME AVEC ZERO SHOT INFERENCE:\\n{output}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgRarv0V1q6J"
      },
      "source": [
        "C'est déjà mieux ! Mais on peut encore faire mieux. Essayons de rajouter un exemple de résumé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6V_Nwkk1q6J"
      },
      "source": [
        "### 2. One Shot Inference\n",
        "\n",
        "L'inférence one-shot et few-shot consiste à fournir au modèle de langage un ou plusieurs exemples complets de paires consigne-réponse correspondant à votre tâche avant de lui soumettre la consigne réelle que vous souhaitez qu'il accomplisse. Cela s'appelle \"l'apprentissage en contexte\" (_in context learning_), et cela permet au modèle de comprendre votre tâche spécifique. Pour en savoir plus, vous pouvez consulter [cet article](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zy4cIWk1q6J"
      },
      "source": [
        "Définissons une fonction qui permet de générer un prompt avec 1 exemple de dialogue et son résumé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoheB97o1q6K"
      },
      "outputs": [],
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06opn_51q6K"
      },
      "source": [
        "Construsons le prompt et affichons le."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmI1RI8M1q6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b853096-c3db-41f9-a5ae-8fddd1c35269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: What time is it, Tom?\n",
            "#Person2#: Just a minute. It's ten to nine by my watch.\n",
            "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
            "#Person2#: What's the hurry?\n",
            "#Person1#: I must catch the nine-thirty train.\n",
            "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
            "\n",
            "Summary:\n",
            "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Have you considered upgrading your system?\n",
            "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
            "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
            "#Person2#: That would be a definite bonus.\n",
            "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
            "#Person2#: How can we do that?\n",
            "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
            "#Person2#: No.\n",
            "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
            "#Person2#: That sounds great. Thanks.\n",
            "\n",
            "Summary:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_indices_full = [40]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "# one_shot_prompt is a string\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7KszbBM1q6K"
      },
      "source": [
        "Lançons l'inférence sur un dialogue, qui doit bien entendu être différent de celui utilisé pour réaliser l'exemple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nebu4Fyu1q6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e910f16-06c3-45d3-ceaf-2c79f5210829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "RESME HUMAIN:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            ">>>RESME LLM AVEC ONE SHOT INFERENCE:\n",
            "#Person1#: You could add a painting program to your software. #Person2#: That would be a bonus. #Person1#: You might also want to upgrade your hardware because it is pretty outdated now\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'RESME HUMAIN:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'>>>RESME LLM AVEC ONE SHOT INFERENCE:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAuj-S041q6K"
      },
      "source": [
        "Voilà qui est encore mieux !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeTXad5B1q6K"
      },
      "source": [
        "### 3. Few shot inference\n",
        "\n",
        "Essayons à présent de fournir 3 exemples de paires (dialogue, résumé). C'est ce que l'on appelle le **few shot inference**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OAhJRTR1q6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec343c3-a654-4c65-972a-86d3775c782c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Stupid girl, making me spend so much money, now I have to get it from the ATM...\n",
            "#Person2#: Hello, welcome to Universal Bank. Please insert your card into the slot.\n",
            "#Person1#: I know where to put my card! Stupid machine, talking to me like I ' m an idiot...\n",
            "#Person2#: Please input your 6 digit PIN code followed by the pound key. Thank you. Please select an option. Thank you. You have selected withdrawal.\n",
            "#Person1#: Yeah, yeah, I know what I selected. Just gimme my money!\n",
            "#Person2#: Please type the amount you would like to withdraw. Thank you, you want to transfer 10000 USD to the World Wildlife Foundation. If this is correct please press 1.\n",
            "#Person1#: No, no! Stupid machine, what are you doing! No!\n",
            "#Person2#: Confirmed. Thank you for using our bank! Please remove your card from the slot. Goodbye!\n",
            "#Person1#: No, no way! What happened? Give me my money!\n",
            "#Person2#: Danger, danger! The exits have been sealed and the doors will remain locked in until the local authorities arrive. Thank you for using our bank. Have a nice day.\n",
            "\n",
            "Summary:\n",
            "#Person1# wants to withdraw money from an ATM, but the ATM automatically transfers 10000 USD to the World Wildlife Foundation after confirming the withdrawal option. #Person1# gets mad and somehow locked in.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Stupid girl, making me spend so much money, now I have to get it from the ATM...\n",
            "#Person2#: Hello, welcome to Universal Bank. Please insert your card into the slot.\n",
            "#Person1#: I know where to put my card! Stupid machine, talking to me like I ' m an idiot...\n",
            "#Person2#: Please input your 6 digit PIN code followed by the pound key. Thank you. Please select an option. Thank you. You have selected withdrawal.\n",
            "#Person1#: Yeah, yeah, I know what I selected. Just gimme my money!\n",
            "#Person2#: Please type the amount you would like to withdraw. Thank you, you want to transfer 10000 USD to the World Wildlife Foundation. If this is correct please press 1.\n",
            "#Person1#: No, no! Stupid machine, what are you doing! No!\n",
            "#Person2#: Confirmed. Thank you for using our bank! Please remove your card from the slot. Goodbye!\n",
            "#Person1#: No, no way! What happened? Give me my money!\n",
            "#Person2#: Danger, danger! The exits have been sealed and the doors will remain locked in until the local authorities arrive. Thank you for using our bank. Have a nice day.\n",
            "\n",
            "Summary:\n",
            "#Person1# run out of money because of a girl, and is withdrawing money from an ATM. But the ATM seems to go wrong and transfers #Person1#'s money to the World Wildlife Foundation, driving #Person1# crazy.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Stupid girl, making me spend so much money, now I have to get it from the ATM...\n",
            "#Person2#: Hello, welcome to Universal Bank. Please insert your card into the slot.\n",
            "#Person1#: I know where to put my card! Stupid machine, talking to me like I ' m an idiot...\n",
            "#Person2#: Please input your 6 digit PIN code followed by the pound key. Thank you. Please select an option. Thank you. You have selected withdrawal.\n",
            "#Person1#: Yeah, yeah, I know what I selected. Just gimme my money!\n",
            "#Person2#: Please type the amount you would like to withdraw. Thank you, you want to transfer 10000 USD to the World Wildlife Foundation. If this is correct please press 1.\n",
            "#Person1#: No, no! Stupid machine, what are you doing! No!\n",
            "#Person2#: Confirmed. Thank you for using our bank! Please remove your card from the slot. Goodbye!\n",
            "#Person1#: No, no way! What happened? Give me my money!\n",
            "#Person2#: Danger, danger! The exits have been sealed and the doors will remain locked in until the local authorities arrive. Thank you for using our bank. Have a nice day.\n",
            "\n",
            "Summary:\n",
            "#Person1# is withdrawing money from an ATM. But the ATM wrongly transfers #Person1#'s money to the World Wildlife Foundation. It drives #Person1# crazy.\n",
            "\n",
            "\n",
            "\n",
            "Dialogue:\n",
            "\n",
            "#Person1#: Where to, miss?\n",
            "#Person2#: Hi! Crenshaw and Hawthorne, at the Holiday Inn that is on that corner.\n",
            "#Person1#: Sure thing. So, where are you flying in from?\n",
            "#Person2#: From China.\n",
            "#Person1#: Really? You don't look very Chinese to me, if you don't mind me saying so.\n",
            "#Person2#: It's fine. I am actually from Mexico. I was in China on a business trip, visiting some local companies that manufacture bathroom products.\n",
            "#Person1#: Wow sounds interesting! Excuse me if I am being a bit nosy but, how old are you?\n",
            "#Person2#: Don't you know it's rude to ask a lady her age?\n",
            "#Person1#: Don't get me wrong! It's just that you seem so young and already doing business overseas!\n",
            "#Person2#: Well thank you! In that case, I am 26 years old, and what about yourself?\n",
            "#Person1#: I am 40 years old and was born and raised here in the good old U. S of A, although I have some Colombian heritage.\n",
            "#Person2#: Really? That's great! Do you speak some Spanish?\n",
            "#Person1#: Uh. . . yeah. . of course!\n",
            "#Person2#: Que bien! Sentences poems habeas en espanol!\n",
            "\n",
            "Summary:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example_indices_full = [84, 85, 86] # exemples à fournir\n",
        "example_index_to_summarize = 201 # dialogue à résumer\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5--xqFeY1q6K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bf7e44-bef1-42b4-9cb2-c7f85717b528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "RESUME HUMAIN:\n",
            "#Person1# is driving #Person2# to an inn. They talk about their careers, ages, and where they was born.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            ">>>RESUME LLM AVEC FEW SHOT INFERENCE:\n",
            "#Person1 is flying to China from Mexico. #Person2 is from Mexico. #Person1 is from Mexico. #Person2 is from Mexico. #Person1 is from China. #Person2 is from Mexico\n"
          ]
        }
      ],
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'RESUME HUMAIN:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'>>>RESUME LLM AVEC FEW SHOT INFERENCE:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-PSx3t81q6K"
      },
      "source": [
        "**Exercice** : modifier les exemples fournis en entrée et indiquer ce que vous contacter en commentaire dans une section markdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rup1JDmD1q6K"
      },
      "source": [
        "## Influence des paramètres du LLM\n",
        "\n",
        "Nous allons maintenant faire varier plusieurs paramètres du LLM :\n",
        "- température\n",
        "- top_k\n",
        "- top_p\n",
        "- sampling\n",
        "\n",
        "Pour cela aidez-vous de la documentation :\n",
        "- https://huggingface.co/docs/transformers/generation_strategies\n",
        "- https://huggingface.co/docs/transformers/main_classes/text_generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV4sUFTY1q6K"
      },
      "source": [
        "**Exercice** : créer une fonction qui prend les 4 paramètres ci-dessous et le paramètres _few_shot_prompt_ défini précédemment et qui retourne le résultat de la génération."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6a6brPv1q6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2238cf3a-bc29-4c80-a417-c6b1a065c5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1 is flying to China from Mexico. #Person2 is from Mexico.\n"
          ]
        }
      ],
      "source": [
        "def generate_summary(temperature, top_k, top_p, sampling, prompt):\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        do_sample=sampling\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            generation_config=generation_config,\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return output\n",
        "\n",
        "print(generate_summary(0.2, 50, 0.9, True, few_shot_prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P_b_6xh1q6N"
      },
      "source": [
        "**Exercice** : expliquer en 1 ou 2 lignes l'influence de chacun des 4 paramètres (dans une section markdown)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}